{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67158,"databundleVersionId":7460879,"sourceType":"competition"},{"sourceId":7548674,"sourceType":"datasetVersion","datasetId":4396290}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoImageProcessor, BeitModel\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom transformers import BeitImageProcessor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"********Biet**","metadata":{}},{"cell_type":"code","source":"src_dir = '/kaggle/input/image-search/test/images'\nquery_dir = '/kaggle/input/image-search/queries/queries'\nsubmission = pd.read_csv('/kaggle/input/image-search/sample_submission.csv')\nmodel = BeitModel.from_pretrained(\"Raghavan/beit3_base_patch16_480_vqa\").cuda().eval()\nprocessor = BeitImageProcessor.from_pretrained(\"Raghavan/beit3_base_patch16_480_vqa\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T07:23:39.523899Z","iopub.execute_input":"2024-02-03T07:23:39.524813Z","iopub.status.idle":"2024-02-03T07:24:05.218210Z","shell.execute_reply.started":"2024-02-03T07:23:39.524776Z","shell.execute_reply":"2024-02-03T07:24:05.217409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['dot_class'] = 22\nsubmission['cosine_class'] = 22\nwith torch.no_grad():\n    query_images = []\n    query_classes = []\n    for file in os.listdir(query_dir):\n        inputs = processor(images=[Image.open(os.path.join(query_dir, file)).convert('RGB')], return_tensors='pt').to('cuda')\n        outputs = model.forward(inputs.pixel_values)\n        outputs = outputs.pooler_output\n        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n        query_images.append(outputs)\n        query_classes.append(int(file[:-4]))\n    query_images = torch.cat(query_images)\n    for idx, row in submission.iterrows():\n        if not pd.isna(row['class']):\n            continue\n        inputs = processor(images=[Image.open(os.path.join(src_dir, row['img_file'])).convert('RGB')], return_tensors='pt').to('cuda')\n        outputs = outputs = model.forward(inputs.pixel_values)\n        outputs = outputs.pooler_output\n        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n        values = outputs @ query_images.T\n        if values.softmax(1).max() > .05:\n            submission.at[idx, 'dot_class'] = query_classes[values.cpu().argmax().numpy().tolist()]\n        cosine = torch.cosine_similarity(outputs, query_images)\n        if cosine.max() > 0.8:\n            submission.at[idx, 'cosine_class'] = query_classes[cosine.cpu().argmax().numpy().tolist()]\n\n    sub = submission[['img_file',]]\n    sub['class'] = submission['dot_class']\n    sub.to_csv('dot_product.csv', index=False)\n    sub['class'] = submission['cosine_class']\n    sub.to_csv('cosine_similarity.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T07:24:31.811052Z","iopub.execute_input":"2024-02-03T07:24:31.811675Z","iopub.status.idle":"2024-02-03T07:25:40.633074Z","shell.execute_reply.started":"2024-02-03T07:24:31.811628Z","shell.execute_reply":"2024-02-03T07:25:40.632174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SigLip","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModel,\n    CLIPProcessor,AutoProcessor\n)\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torch\n\n\nsrc_dir = '/kaggle/input/image-search/test/images'\nquery_dir = '/kaggle/input/quries/queries'\nsubmission = pd.read_csv('/kaggle/input/image-search/sample_submission.csv')\nmodel = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda().eval()\nprocessor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n\nsubmission['dot_class'] = 22\nsubmission['cosine_class'] = 22\nwith torch.no_grad():\n    query_images = []\n    query_classes = []\n    for file in os.listdir(query_dir):\n        inputs = processor(images=[Image.open(os.path.join(query_dir, file)).convert('RGB')], return_tensors='pt').to('cuda')\n        outputs = model.get_image_features(inputs.pixel_values).cpu()\n        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n        query_images.append(outputs)\n        query_classes.append(int(file[:-4]))\n    query_images = torch.cat(query_images)\n    for idx, row in submission.iterrows():\n        if not pd.isna(row['class']):\n            continue\n        inputs = processor(images=[Image.open(os.path.join(src_dir, row['img_file'])).convert('RGB')], return_tensors='pt').to('cuda')\n        outputs = model.get_image_features(inputs.pixel_values).cpu()\n        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n        values = outputs @ query_images.T\n        if values.softmax(1).max() > .05:\n            submission.at[idx, 'dot_class'] = query_classes[values.argmax().numpy().tolist()]\n        cosine = torch.cosine_similarity(outputs, query_images)\n        if cosine.max() > 0.75:\n            submission.at[idx, 'cosine_class'] = query_classes[cosine.argmax().numpy().tolist()]\n\n    sub = submission[['img_file',]]\n    sub['class'] = submission['dot_class']\n    sub.to_csv('dot_product_seg2.csv', index=False)\n    sub['class'] = submission['cosine_class']\n    sub.to_csv('cosine_similarity_seg2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:25:00.209861Z","iopub.execute_input":"2024-02-04T00:25:00.210184Z","iopub.status.idle":"2024-02-04T00:28:21.015662Z","shell.execute_reply.started":"2024-02-04T00:25:00.210158Z","shell.execute_reply":"2024-02-04T00:28:21.014654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try Code","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModel,\n    CLIPProcessor,AutoProcessor\n)\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:31:01.569014Z","iopub.execute_input":"2024-02-03T14:31:01.569714Z","iopub.status.idle":"2024-02-03T14:31:01.589800Z","shell.execute_reply.started":"2024-02-03T14:31:01.569682Z","shell.execute_reply":"2024-02-03T14:31:01.588946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src_dir = '/kaggle/input/image-search/test/images/'\nquery_dir = '/kaggle/input/image-search/queries/queries'\nsubmission = pd.read_csv('/kaggle/input/image-search/sample_submission.csv')\nmodel = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda().eval()\nprocessor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfile = \"599a1cad-7536-4150-b502-cb5720a4b82e.jpg\"\nprocessor(images=[Image.open(os.path.join(src_dir, file)).convert('RGB')], return_tensors='pt').to('cuda')\noutputs = model.get_image_features(inputs.pixel_values).cpu()\noutputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\nvalues = outputs @ query_images.T\ncosine = torch.cosine_similarity(outputs, query_images)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:07:10.954768Z","iopub.status.idle":"2024-02-04T00:07:10.955135Z","shell.execute_reply.started":"2024-02-04T00:07:10.954958Z","shell.execute_reply":"2024-02-04T00:07:10.954973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:51.491181Z","iopub.status.idle":"2024-02-04T00:06:51.491524Z","shell.execute_reply.started":"2024-02-04T00:06:51.491357Z","shell.execute_reply":"2024-02-04T00:06:51.491372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:51.492784Z","iopub.status.idle":"2024-02-04T00:06:51.493244Z","shell.execute_reply.started":"2024-02-04T00:06:51.493011Z","shell.execute_reply":"2024-02-04T00:06:51.493031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:51.666751Z","iopub.execute_input":"2024-02-04T00:06:51.667055Z","iopub.status.idle":"2024-02-04T00:06:51.674129Z","shell.execute_reply.started":"2024-02-04T00:06:51.667028Z","shell.execute_reply":"2024-02-04T00:06:51.673170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine.max()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:52.047797Z","iopub.execute_input":"2024-02-04T00:06:52.048179Z","iopub.status.idle":"2024-02-04T00:06:52.055318Z","shell.execute_reply.started":"2024-02-04T00:06:52.048149Z","shell.execute_reply":"2024-02-04T00:06:52.054306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine.argmax().numpy().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:52.946717Z","iopub.execute_input":"2024-02-04T00:06:52.947549Z","iopub.status.idle":"2024-02-04T00:06:52.954028Z","shell.execute_reply.started":"2024-02-04T00:06:52.947512Z","shell.execute_reply":"2024-02-04T00:06:52.952951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if cosine.max() > 0.8:\n    submission.at[idx, 'cosine_class'] = query_classes[cosine.argmax().numpy().tolist()]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:53.741555Z","iopub.execute_input":"2024-02-04T00:06:53.742158Z","iopub.status.idle":"2024-02-04T00:06:53.747221Z","shell.execute_reply.started":"2024-02-04T00:06:53.742124Z","shell.execute_reply":"2024-02-04T00:06:53.746145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\n\n# Replace 'path/to/your/image.jpg' with the actual path to your image file\nimage_path = '/kaggle/input/image-search/test/images/e4f64bb3-e7d0-4cee-9846-1ed53324a4a5.jpg'\n\n# Display the image\nImage(filename=image_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T00:06:54.926509Z","iopub.execute_input":"2024-02-04T00:06:54.927370Z","iopub.status.idle":"2024-02-04T00:06:54.934879Z","shell.execute_reply.started":"2024-02-04T00:06:54.927327Z","shell.execute_reply":"2024-02-04T00:06:54.933821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess query","metadata":{}},{"cell_type":"code","source":"!pip install super-image","metadata":{"execution":{"iopub.status.busy":"2024-02-03T10:34:07.328577Z","iopub.execute_input":"2024-02-03T10:34:07.329306Z","iopub.status.idle":"2024-02-03T10:34:22.387731Z","shell.execute_reply.started":"2024-02-03T10:34:07.329273Z","shell.execute_reply":"2024-02-03T10:34:22.386605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try to Improve queries ","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom torchvision.transforms import ToTensor\nfrom super_image import EdsrModel, ImageLoader\n\n# Replace 'path/to/your/image.jpg' with the actual path to your image file\nimage_path = '/kaggle/input/image-search/queries/queries/18.jpg'\n\n# Load the image using PIL\nimage = Image.open(image_path).convert('RGB')\n\n# Transform the image to tensor\ninput_tensor = ToTensor()(image).unsqueeze(0)\n\n# Load the EDSR model\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr', scale=4)  # Replace with the desired scale\n\n# Make predictions\nwith torch.no_grad():\n    preds = model(input_tensor)\n\n# Display the original and upscaled images (optional)\nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\n\nto_pil = ToPILImage()\nplt.subplot(1, 2, 1)\nplt.title('Original Image')\nplt.imshow(to_pil(input_tensor.squeeze(0)))\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.title('Upscaled Image')\nplt.imshow(to_pil(preds.squeeze(0)))\nplt.axis('off')\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T10:56:01.484005Z","iopub.execute_input":"2024-02-03T10:56:01.484771Z","iopub.status.idle":"2024-02-03T10:58:05.303489Z","shell.execute_reply.started":"2024-02-03T10:56:01.484738Z","shell.execute_reply":"2024-02-03T10:58:05.302511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nfrom collections import Counter\n\ndef count_labels(csv_file_path, label_column):\n    # Initialize a Counter to count occurrences of each label\n    label_counter = Counter()\n\n    # Read the CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.DictReader(csv_file)\n        \n        # Iterate through rows and count labels\n        for row in reader:\n            label = row[label_column]\n            label_counter[label] += 1\n\n    # Print the label counts\n    for label, count in label_counter.items():\n        print(f'{label}: {count}')\n\n# Example usage\ncsv_file_path = '/kaggle/working/cosine_similarity.csv'\nlabel_column_name = 'class'  # Replace with the actual column name containing class labels\ncount_labels(csv_file_path, label_column_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T11:04:57.121008Z","iopub.execute_input":"2024-02-03T11:04:57.121999Z","iopub.status.idle":"2024-02-03T11:04:57.134150Z","shell.execute_reply.started":"2024-02-03T11:04:57.121964Z","shell.execute_reply":"2024-02-03T11:04:57.133061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_labels(\"/kaggle/working/cosine_similarity1.csv\", \"class\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T11:17:48.655124Z","iopub.execute_input":"2024-02-03T11:17:48.655912Z","iopub.status.idle":"2024-02-03T11:17:48.665744Z","shell.execute_reply.started":"2024-02-03T11:17:48.655877Z","shell.execute_reply":"2024-02-03T11:17:48.664699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Read the image\nimage = cv2.imread('/kaggle/input/image-search/queries/queries/0.jpg')\n\n# Define a kernel for morphological operations\nkernel = np.ones((3, 3), np.uint8)\n\n# Apply closing operation\n\nimage_dialte = cv2.dilate(image,kernel)\nimage_erode = cv2.erode(image,kernel)\nclosing_result = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\nopen_result = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n\n# Display the original and the result\nplt.figure(figsize=(20, 5))\n\n# Plot each image in a separate subplot\nplt.subplot(1, 5, 1)\nplt.imshow(image)\nplt.axis('off')  # Turn off axis labels if you don't need them\n\nplt.subplot(1, 5, 2)\nplt.imshow(image_dialte)\nplt.axis('off')\n\nplt.subplot(1, 5, 3)\nplt.imshow(image_erode)\nplt.axis('off')\n\nplt.subplot(1, 5, 4)\nplt.imshow(closing_result)\nplt.axis('off')\n\nplt.subplot(1, 5, 5)\nplt.imshow(open_result)\nplt.axis('off')\n\n# Adjust layout for better spacing\nplt.tight_layout()\n\n# Show the figure\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T12:43:34.820141Z","iopub.execute_input":"2024-02-03T12:43:34.820598Z","iopub.status.idle":"2024-02-03T12:43:35.289109Z","shell.execute_reply.started":"2024-02-03T12:43:34.820562Z","shell.execute_reply":"2024-02-03T12:43:35.288047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"open_result = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n#plt.imshow(image)\n# plt.imshow(image_dialte)\n# plt.imshow(image_erode)\n#plt.imshow(closing_result)\nplt.imshow(open_result)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T12:43:39.422829Z","iopub.execute_input":"2024-02-03T12:43:39.423508Z","iopub.status.idle":"2024-02-03T12:43:39.646486Z","shell.execute_reply.started":"2024-02-03T12:43:39.423476Z","shell.execute_reply":"2024-02-03T12:43:39.645558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Specify the path for the new folder\nnew_folder_path = '/kaggle/working/queries/'\n\n# Create the new folder\nos.makedirs(new_folder_path, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T12:33:16.051847Z","iopub.execute_input":"2024-02-03T12:33:16.052239Z","iopub.status.idle":"2024-02-03T12:33:16.057710Z","shell.execute_reply.started":"2024-02-03T12:33:16.052211Z","shell.execute_reply":"2024-02-03T12:33:16.056599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport os\n\n# Read the old picture file\nfolder = '/kaggle/input/image-search/queries/queries/'\nfile_name = '4.jpg'\nold_picture = cv2.imread(folder + file_name)\n\n# Specify the folder where you want to save the new picture\noutput_folder = '/kaggle/working/queries/'\n\n# Create the output folder if it doesn't exist\nos.makedirs(output_folder, exist_ok=True)\n\n# Save the modified picture to the specified folder\ncv2.imwrite(os.path.join(output_folder, file_name ), old_picture)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T12:37:00.093289Z","iopub.execute_input":"2024-02-03T12:37:00.094233Z","iopub.status.idle":"2024-02-03T12:37:00.106038Z","shell.execute_reply.started":"2024-02-03T12:37:00.094198Z","shell.execute_reply":"2024-02-03T12:37:00.105019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.remove(\"/kaggle/working/queries/new_picture.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T12:37:54.657820Z","iopub.execute_input":"2024-02-03T12:37:54.658241Z","iopub.status.idle":"2024-02-03T12:37:54.663489Z","shell.execute_reply.started":"2024-02-03T12:37:54.658208Z","shell.execute_reply":"2024-02-03T12:37:54.662457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_labels(\"/kaggle/working/cosine_similarity1.csv\", \"class\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T11:51:51.484994Z","iopub.execute_input":"2024-02-03T11:51:51.486025Z","iopub.status.idle":"2024-02-03T11:51:51.538618Z","shell.execute_reply.started":"2024-02-03T11:51:51.485987Z","shell.execute_reply":"2024-02-03T11:51:51.537244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SAM (Segmentation )","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\nimport sys\n!{sys.executable} -m pip install opencv-python matplotlib\n!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n\n!mkdir images\n!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n\n!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth","metadata":{"execution":{"iopub.status.busy":"2024-02-03T20:20:05.794927Z","iopub.execute_input":"2024-02-03T20:20:05.795280Z","iopub.status.idle":"2024-02-03T20:21:00.631012Z","shell.execute_reply.started":"2024-02-03T20:20:05.795251Z","shell.execute_reply":"2024-02-03T20:21:00.629880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:20:58.417216Z","iopub.execute_input":"2024-02-03T15:20:58.418965Z","iopub.status.idle":"2024-02-03T15:20:58.579489Z","shell.execute_reply.started":"2024-02-03T15:20:58.418909Z","shell.execute_reply":"2024-02-03T15:20:58.578701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('/kaggle/input/image-search/queries/queries/4.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:42:06.752368Z","iopub.execute_input":"2024-02-03T16:42:06.753000Z","iopub.status.idle":"2024-02-03T16:42:06.764776Z","shell.execute_reply.started":"2024-02-03T16:42:06.752959Z","shell.execute_reply":"2024-02-03T16:42:06.763739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.imshow(image)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:42:12.539602Z","iopub.execute_input":"2024-02-03T16:42:12.539993Z","iopub.status.idle":"2024-02-03T16:42:12.808742Z","shell.execute_reply.started":"2024-02-03T16:42:12.539963Z","shell.execute_reply":"2024-02-03T16:42:12.807721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\nmask_generator = SamAutomaticMaskGenerator(sam)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:21:19.802227Z","iopub.execute_input":"2024-02-03T15:21:19.802580Z","iopub.status.idle":"2024-02-03T15:21:28.766878Z","shell.execute_reply.started":"2024-02-03T15:21:19.802554Z","shell.execute_reply":"2024-02-03T15:21:28.766072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n        img[m] = color_mask\n    ax.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:55:51.309975Z","iopub.execute_input":"2024-02-03T14:55:51.310348Z","iopub.status.idle":"2024-02-03T14:55:51.317547Z","shell.execute_reply.started":"2024-02-03T14:55:51.310321Z","shell.execute_reply":"2024-02-03T14:55:51.316428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install datasets open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:02:23.905446Z","iopub.execute_input":"2024-02-03T16:02:23.906212Z","iopub.status.idle":"2024-02-03T16:02:36.156768Z","shell.execute_reply.started":"2024-02-03T16:02:23.906178Z","shell.execute_reply":"2024-02-03T16:02:36.155452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load clip\n#\"google/siglip-so400m-patch14-384\"\ndef load_clip(model_name: str = \"ViT-L-14-336\",\n              pretrained_name: str = \"openai\"):\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n                                                               device = device,\n                                                               pretrained = pretrained_name)\n  # model.load_state_dict(torch.load(pretrained_name,\n  #                                  map_location = \"cpu\")[\"state_dict\"])\n  return model.to(device), preprocess","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:05:04.784794Z","iopub.execute_input":"2024-02-03T16:05:04.785178Z","iopub.status.idle":"2024-02-03T16:05:04.790836Z","shell.execute_reply.started":"2024-02-03T16:05:04.785150Z","shell.execute_reply":"2024-02-03T16:05:04.789872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport urllib\nfrom functools import lru_cache\nfrom random import randint\nfrom typing import Any, Callable, Dict, List, Tuple\n\nimport open_clip\nimport cv2\nimport numpy as np\nimport PIL\nimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import SamAutomaticMaskGenerator, sam_model_registry","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:22:54.213091Z","iopub.execute_input":"2024-02-03T15:22:54.213477Z","iopub.status.idle":"2024-02-03T15:22:56.051996Z","shell.execute_reply.started":"2024-02-03T15:22:54.213443Z","shell.execute_reply":"2024-02-03T15:22:56.051174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CHECKPOINT_PATH = \"sam_vit_h_4b8939.pth\"\nCHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\nMODEL_TYPE = \"default\"\nMAX_WIDTH = MAX_HEIGHT = 400\nTHRESHOLD = 0.05\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:10:46.898437Z","iopub.execute_input":"2024-02-03T15:10:46.898752Z","iopub.status.idle":"2024-02-03T15:10:46.903913Z","shell.execute_reply.started":"2024-02-03T15:10:46.898726Z","shell.execute_reply":"2024-02-03T15:10:46.902924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_image_size(image: np.ndarray) -> np.ndarray:\n    height, width = image.shape[:2]\n    MAX_WIDTH = MAX_HEIGHT = 400\n\n    if height > width:\n        if height > MAX_HEIGHT:\n              height, width = MAX_HEIGHT, int(MAX_HEIGHT / height * width)\n    else:\n        if width > MAX_WIDTH:\n            height, width = int(MAX_WIDTH / width * height), MAX_WIDTH\n    image = cv2.resize(image, (width, height))\n    return image\n\n@torch.no_grad()\ndef get_similarity_score(crop_images: List[PIL.Image.Image], prompt: str) -> torch.Tensor:\n    from open_clip import tokenizer\n\n    model, preprocess = load_clip()\n    image_input = [preprocess(crop) for crop in crop_images]\n    image_input = torch.stack(image_input).to(device)\n    text_input = tokenizer.tokenize(prompt).to(device)\n    with torch.no_grad():\n        image_features = model.encode_image(image_input).float()\n        text_features = model.encode_text(text_input).float()\n\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    similarity = (image_features @ text_features.T)\n    similarity = similarity.detach().cpu().numpy()\n    print(\"max: \", np.max(similarity), \"ave: \", np.mean(similarity))\n    return similarity\n\ndef filter_masks(image: np.ndarray, masks: List[Dict[str, Any]],\n                 predicted_iou_threshold: float, stability_score_threshold: float,\n                 prompt: str, clip_threshold: float) -> List[Dict[str, Any]]:\n\n    cropped_masks: List[PIL.Image.Image] = []\n    filtered_masks: List[Dict[str, Any]] = []\n\n    for mask in masks:\n        if (mask[\"predicted_iou\"] < predicted_iou_threshold) or (mask[\"stability_score\"] < stability_score_threshold):\n            continue\n        filtered_masks.append(mask)\n\n        # crop image from mask\n        x, y, w, h = mask[\"bbox\"]\n        masked = image * np.expand_dims(mask[\"segmentation\"], -1)\n        crop = masked[y: y + h, x: x + w]\n        crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n        crop = PIL.Image.fromarray(crop)\n        cropped_masks.append(crop)\n\n        if prompt and filtered_masks:\n            most_similar_masks = []\n            scores = get_similarity_score(cropped_masks, prompt)\n            for i, score in enumerate(scores):\n                if score > clip_threshold:\n                      most_similar_masks.append(filtered_masks[i])\n\n    return most_similar_masks\n\ndef draw_masks(image: np.ndarray, masks: List[np.ndarray],\n               alpha: float = 0.7) -> np.ndarray:\n    for mask in masks:\n        color = [randint(127, 255) for _ in range(3)]\n\n        # draw mask\n        colored_mask = np.expand_dims(mask[\"segmentation\"], 0).repeat(3, axis=0)\n        colored_mask = np.moveaxis(colored_mask, 0, -1)\n        masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n        image_overlay = masked.filled()\n        image = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n\n        # draw contour\n        contours, _ = cv2.findContours(\n            np.uint8(mask[\"segmentation\"]), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n        cv2.drawContours(image, contours, -1, (255, 0, 0), 1)\n\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:59:50.581294Z","iopub.execute_input":"2024-02-03T15:59:50.581994Z","iopub.status.idle":"2024-02-03T15:59:50.599282Z","shell.execute_reply.started":"2024-02-03T15:59:50.581962Z","shell.execute_reply":"2024-02-03T15:59:50.598370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:44:06.272882Z","iopub.execute_input":"2024-02-03T16:44:06.274231Z","iopub.status.idle":"2024-02-03T16:44:06.484300Z","shell.execute_reply.started":"2024-02-03T16:44:06.274192Z","shell.execute_reply":"2024-02-03T16:44:06.483350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks = mask_generator.generate(image)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:43:56.586644Z","iopub.execute_input":"2024-02-03T16:43:56.587084Z","iopub.status.idle":"2024-02-03T16:44:02.093980Z","shell.execute_reply.started":"2024-02-03T16:43:56.587054Z","shell.execute_reply":"2024-02-03T16:44:02.092976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = adjust_image_size(image)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:44:10.865600Z","iopub.execute_input":"2024-02-03T16:44:10.866321Z","iopub.status.idle":"2024-02-03T16:44:10.870755Z","shell.execute_reply.started":"2024-02-03T16:44:10.866286Z","shell.execute_reply":"2024-02-03T16:44:10.869950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_iou_threshold = 0.5\nstability_score_threshold = 0.8\nclip_threshold = 0.24\nprompt = \"Logo\"","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:44:13.028659Z","iopub.execute_input":"2024-02-03T16:44:13.029250Z","iopub.status.idle":"2024-02-03T16:44:13.033680Z","shell.execute_reply.started":"2024-02-03T16:44:13.029220Z","shell.execute_reply":"2024-02-03T16:44:13.032784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filter_mask = filter_masks(\n      image,\n      masks,\n      predicted_iou_threshold,\n      stability_score_threshold,\n      prompt,\n      clip_threshold,\n  )","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:46:49.871181Z","iopub.execute_input":"2024-02-03T16:46:49.871591Z","iopub.status.idle":"2024-02-03T16:46:49.934030Z","shell.execute_reply.started":"2024-02-03T16:46:49.871563Z","shell.execute_reply":"2024-02-03T16:46:49.932842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_draw = draw_masks(image,filter_mask )\nimage_draw = cv2.cvtColor(image_draw, cv2.COLOR_BGR2RGB)\nplt.imshow(image_draw)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:29:23.142888Z","iopub.execute_input":"2024-02-03T16:29:23.143269Z","iopub.status.idle":"2024-02-03T16:29:23.362810Z","shell.execute_reply.started":"2024-02-03T16:29:23.143239Z","shell.execute_reply":"2024-02-03T16:29:23.361769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Load the original image and segmentation mask\noriginal_image = cv2.imread('/kaggle/input/image-search/queries/queries/0.jpg')\nsegmentation_mask = cv2.cvtColor(image_draw, cv2.COLOR_BGR2GRAY)\n\n\n# Ensure the mask is binary (0 or 255)\n_, segmentation_mask = cv2.threshold(segmentation_mask, 127, 255, cv2.THRESH_BINARY)\n\n# Invert the mask\ninverse_mask = cv2.bitwise_not(segmentation_mask)\n\n# Apply the inverse mask to get the regions not segmented\nresult_image = cv2.bitwise_and(original_image, original_image, mask=inverse_mask)\n\n# Save or display the result\nplt.imshow(result_image)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:38:29.783073Z","iopub.execute_input":"2024-02-03T16:38:29.783902Z","iopub.status.idle":"2024-02-03T16:38:30.014239Z","shell.execute_reply.started":"2024-02-03T16:38:29.783871Z","shell.execute_reply":"2024-02-03T16:38:30.012998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_draw = image_draw\n\n# Convert the colored image to grayscale for segmentation mask\nsegmentation_mask = cv2.cvtColor(image_draw, cv2.COLOR_BGR2GRAY)\n\n# Ensure the mask is binary (0 or 255)\n_, segmentation_mask = cv2.threshold(segmentation_mask, 127, 255, cv2.THRESH_BINARY)\n\n# Invert the mask\ninverse_mask = cv2.bitwise_not(segmentation_mask)\n\n# Assuming you have the original image loaded as well (original_image)\noriginal_image = cv2.imread('/kaggle/input/image-search/queries/queries/0.jpg')\n\n# Apply the inverse mask to get the regions not segmented\nresult_image = cv2.bitwise_and(original_image, original_image, mask=inverse_mask)\nplt.imshow(result_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:41:32.894537Z","iopub.execute_input":"2024-02-03T16:41:32.895385Z","iopub.status.idle":"2024-02-03T16:41:33.103983Z","shell.execute_reply.started":"2024-02-03T16:41:32.895349Z","shell.execute_reply":"2024-02-03T16:41:33.103086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MASKS","metadata":{}},{"cell_type":"code","source":"image = cv2.imread('/kaggle/input/image-search/queries/queries/11.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:57:01.382346Z","iopub.execute_input":"2024-02-03T16:57:01.383253Z","iopub.status.idle":"2024-02-03T16:57:01.391543Z","shell.execute_reply.started":"2024-02-03T16:57:01.383217Z","shell.execute_reply":"2024-02-03T16:57:01.390526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks = mask_generator.generate(image)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:57:01.691519Z","iopub.execute_input":"2024-02-03T16:57:01.691831Z","iopub.status.idle":"2024-02-03T16:57:07.086753Z","shell.execute_reply.started":"2024-02-03T16:57:01.691805Z","shell.execute_reply":"2024-02-03T16:57:07.085940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_mask_image = draw_masks(image,masks)\nplt.imshow(all_mask_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:57:07.088296Z","iopub.execute_input":"2024-02-03T16:57:07.088586Z","iopub.status.idle":"2024-02-03T16:57:07.249240Z","shell.execute_reply.started":"2024-02-03T16:57:07.088560Z","shell.execute_reply":"2024-02-03T16:57:07.248338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\n\n\n# Specify the input and output folders\ninput_folder = '/kaggle/input/image-search/queries/queries/'\noutput_folder = 'edit_queries/'\n\n# Create the output folder if it doesn't exist\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n\n# Loop through each file in the input folder\nfor filename in os.listdir(input_folder):\n    if filename.endswith(('.jpg', '.png', '.jpeg')):  # Add more file extensions if needed\n        # Load the original image\n        original_image = cv2.imread(os.path.join(input_folder, filename))\n        image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n        masks = mask_generator.generate(image)\n        all_mask_image = draw_masks(image,masks)\n        \n        output_path = os.path.join(output_folder, filename)\n        cv2.imwrite(output_path, all_mask_image)\n\nprint(\"Processing complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:02:45.454510Z","iopub.execute_input":"2024-02-03T17:02:45.454879Z","iopub.status.idle":"2024-02-03T17:05:00.214313Z","shell.execute_reply.started":"2024-02-03T17:02:45.454849Z","shell.execute_reply":"2024-02-03T17:05:00.213310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModel,\n    CLIPProcessor,AutoProcessor\n)\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torch\n\n\nsrc_dir = '/kaggle/input/image-search/test/images'\nquery_dir = '/kaggle/working/edit_queries'\nsubmission = pd.read_csv('/kaggle/input/image-search/sample_submission.csv')\nmodel = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda().eval()\nprocessor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n\nsubmission['dot_class'] = 22\nsubmission['cosine_class'] = 22\nwith torch.no_grad():\n    query_images = []\n    query_classes = []\n    for file in os.listdir(query_dir):\n        inputs = processor(images=[Image.open(os.path.join(query_dir, file)).convert('RGB')], return_tensors='pt').to('cuda')\n        outputs = model.get_image_features(inputs.pixel_values).cpu()\n        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n        query_images.append(outputs)\n        query_classes.append(int(file[:-4]))\n    query_images = torch.cat(query_images)\n    for idx, row in submission.iterrows():\n        if not pd.isna(row['class']):\n            continue\n        inputs = processor(images=[Image.open(os.path.join(src_dir, row['img_file'])).convert('RGB')], return_tensors='pt').to('cuda')\n        outputs = model.get_image_features(inputs.pixel_values).cpu()\n        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n        values = outputs @ query_images.T\n        if values.softmax(1).max() > .05:\n            submission.at[idx, 'dot_class'] = query_classes[values.argmax().numpy().tolist()]\n        cosine = torch.cosine_similarity(outputs, query_images)\n        if cosine.max() > 0.75:\n            submission.at[idx, 'cosine_class'] = query_classes[cosine.argmax().numpy().tolist()]\n\n    sub = submission[['img_file',]]\n    sub['class'] = submission['dot_class']\n    sub.to_csv('dot_product_seg.csv', index=False)\n    sub['class'] = submission['cosine_class']\n    sub.to_csv('cosine_similarity_seg.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:44:28.473351Z","iopub.execute_input":"2024-02-03T18:44:28.473736Z","iopub.status.idle":"2024-02-03T18:45:14.655003Z","shell.execute_reply.started":"2024-02-03T18:44:28.473702Z","shell.execute_reply":"2024-02-03T18:45:14.653655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha = 0.7\ncolor = [randint(127, 255) for _ in range(3)]\n\n# draw mask\ncolored_mask = np.expand_dims(mask[\"segmentation\"], 0).repeat(3, axis=0)\ncolored_mask = np.moveaxis(colored_mask, 0, -1)\nmasked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\nimage_overlay = masked.filled()\nimage = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n\n# draw contour\ncontours, _ = cv2.findContours(\n        np.uint8(mask[\"segmentation\"]), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncv2.drawContours(image, contours, -1, (255, 0, 0), 1)\n\nplt.imshow(image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:53:39.460118Z","iopub.execute_input":"2024-02-03T16:53:39.460491Z","iopub.status.idle":"2024-02-03T16:53:39.670370Z","shell.execute_reply.started":"2024-02-03T16:53:39.460460Z","shell.execute_reply":"2024-02-03T16:53:39.669446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adjust_image = adjust_image_size(image)\nplt.imshow(adjust_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:51:46.234880Z","iopub.execute_input":"2024-02-03T15:51:46.235715Z","iopub.status.idle":"2024-02-03T15:51:46.453101Z","shell.execute_reply.started":"2024-02-03T15:51:46.235680Z","shell.execute_reply":"2024-02-03T15:51:46.452198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(masks))\nprint(masks[0].keys())","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:43:35.340015Z","iopub.execute_input":"2024-02-03T14:43:35.340847Z","iopub.status.idle":"2024-02-03T14:43:35.345685Z","shell.execute_reply.started":"2024-02-03T14:43:35.340814Z","shell.execute_reply":"2024-02-03T14:43:35.344643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"area = []\niou = []\nfor i in range(len(masks)):       \n    area += [masks[i]['area']]\n    iou += [masks[i]['predicted_iou']]\n\n    \nmask =  masks[iou.index(max(ioou))]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:53:02.290954Z","iopub.execute_input":"2024-02-03T14:53:02.291440Z","iopub.status.idle":"2024-02-03T14:53:02.297063Z","shell.execute_reply.started":"2024-02-03T14:53:02.291400Z","shell.execute_reply":"2024-02-03T14:53:02.295956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\na = masks[iou.index(max(iou))]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:00:11.321114Z","iopub.execute_input":"2024-02-03T15:00:11.321485Z","iopub.status.idle":"2024-02-03T15:00:11.327131Z","shell.execute_reply.started":"2024-02-03T15:00:11.321457Z","shell.execute_reply":"2024-02-03T15:00:11.325975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract mask pixel coordinates\nmask_pixel_coords = np.column_stack(np.where(a['segmentation']))\n\n# Create an empty image\nmask_image = Image.new('1', (a['bbox'][2], a['bbox'][3]), 0)\n\n# Set pixels in the mask\nfor coord in mask_pixel_coords:\n    mask_image.putpixel((coord[1], coord[0]), 1)\n\n# Save or display the mask image\n\nmask_image.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T15:00:26.776469Z","iopub.execute_input":"2024-02-03T15:00:26.776886Z","iopub.status.idle":"2024-02-03T15:00:27.143194Z","shell.execute_reply.started":"2024-02-03T15:00:26.776853Z","shell.execute_reply":"2024-02-03T15:00:27.141949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks[area.index(max(area))]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:53:49.120477Z","iopub.execute_input":"2024-02-03T14:53:49.121227Z","iopub.status.idle":"2024-02-03T14:53:49.127972Z","shell.execute_reply.started":"2024-02-03T14:53:49.121194Z","shell.execute_reply":"2024-02-03T14:53:49.126956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.imshow(image)\nshow_anns(masks)\nplt.axis('off')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:43:37.698668Z","iopub.execute_input":"2024-02-03T14:43:37.699277Z","iopub.status.idle":"2024-02-03T14:43:38.221121Z","shell.execute_reply.started":"2024-02-03T14:43:37.699245Z","shell.execute_reply":"2024-02-03T14:43:38.220167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.86,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,  # Requires open-cv to run post-processing\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:43:45.865982Z","iopub.execute_input":"2024-02-03T14:43:45.866737Z","iopub.status.idle":"2024-02-03T14:43:45.873717Z","shell.execute_reply.started":"2024-02-03T14:43:45.866707Z","shell.execute_reply":"2024-02-03T14:43:45.872741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks2 = mask_generator_2.generate(image)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:43:50.437603Z","iopub.execute_input":"2024-02-03T14:43:50.438456Z","iopub.status.idle":"2024-02-03T14:44:00.858926Z","shell.execute_reply.started":"2024-02-03T14:43:50.438422Z","shell.execute_reply":"2024-02-03T14:44:00.858123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.imshow(image)\nshow_anns(masks2)\nplt.axis('off')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:44:01.904944Z","iopub.execute_input":"2024-02-03T14:44:01.905313Z","iopub.status.idle":"2024-02-03T14:44:02.427064Z","shell.execute_reply.started":"2024-02-03T14:44:01.905280Z","shell.execute_reply":"2024-02-03T14:44:02.426136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLIPSeg","metadata":{}},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:55:50.013435Z","iopub.execute_input":"2024-02-03T18:55:50.014173Z","iopub.status.idle":"2024-02-03T18:56:40.240170Z","shell.execute_reply.started":"2024-02-03T18:55:50.014137Z","shell.execute_reply":"2024-02-03T18:56:40.238819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\nimage = Image.open(\"/kaggle/input/image-search/queries/queries/0.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:58:27.650993Z","iopub.execute_input":"2024-02-03T18:58:27.651826Z","iopub.status.idle":"2024-02-03T18:58:27.670290Z","shell.execute_reply.started":"2024-02-03T18:58:27.651793Z","shell.execute_reply":"2024-02-03T18:58:27.669494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n\nprocessor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\nmodel = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:57:40.236659Z","iopub.execute_input":"2024-02-03T18:57:40.237538Z","iopub.status.idle":"2024-02-03T18:57:45.758777Z","shell.execute_reply.started":"2024-02-03T18:57:40.237494Z","shell.execute_reply":"2024-02-03T18:57:45.757963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprompts = [\"LogoBrand\",\"Outer thing\"]\n\ninputs = processor(text=prompts, images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:58:33.163100Z","iopub.execute_input":"2024-02-03T18:58:33.163455Z","iopub.status.idle":"2024-02-03T18:58:33.222381Z","shell.execute_reply.started":"2024-02-03T18:58:33.163427Z","shell.execute_reply":"2024-02-03T18:58:33.221399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport matplotlib.pyplot as plt\n\n# predict\nwith torch.no_grad():\n  outputs = model(**inputs)\n\npreds = outputs.logits.unsqueeze(1)\n\n# visualize prediction\n_, ax = plt.subplots(1, 5, figsize=(15, 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(4)];\n[ax[i+1].text(0, -15, prompts[i]) for i in range(4)];","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:58:41.644805Z","iopub.execute_input":"2024-02-03T18:58:41.645625Z","iopub.status.idle":"2024-02-03T18:58:43.414137Z","shell.execute_reply.started":"2024-02-03T18:58:41.645592Z","shell.execute_reply":"2024-02-03T18:58:43.412888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instance Segmentation (SAM) \n ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-02-03T21:26:36.357768Z","iopub.execute_input":"2024-02-03T21:26:36.358618Z","iopub.status.idle":"2024-02-03T21:26:36.362844Z","shell.execute_reply.started":"2024-02-03T21:26:36.358584Z","shell.execute_reply":"2024-02-03T21:26:36.361954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n\ndef mask_im(mask,  random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    return mask_image\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n    \ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T21:26:36.582650Z","iopub.execute_input":"2024-02-03T21:26:36.583039Z","iopub.status.idle":"2024-02-03T21:26:36.595311Z","shell.execute_reply.started":"2024-02-03T21:26:36.583008Z","shell.execute_reply":"2024-02-03T21:26:36.594389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('/kaggle/input/image-search/queries/queries/14.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\noriginal = image","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:15:32.929061Z","iopub.execute_input":"2024-02-03T23:15:32.929440Z","iopub.status.idle":"2024-02-03T23:15:32.936942Z","shell.execute_reply.started":"2024-02-03T23:15:32.929409Z","shell.execute_reply":"2024-02-03T23:15:32.936185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(image)\nplt.axis('on')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:15:33.942489Z","iopub.execute_input":"2024-02-03T23:15:33.942884Z","iopub.status.idle":"2024-02-03T23:15:34.230946Z","shell.execute_reply.started":"2024-02-03T23:15:33.942851Z","shell.execute_reply":"2024-02-03T23:15:34.229580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamPredictor\n\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\npredictor = SamPredictor(sam)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:15:35.787679Z","iopub.execute_input":"2024-02-03T23:15:35.788330Z","iopub.status.idle":"2024-02-03T23:15:42.265943Z","shell.execute_reply.started":"2024-02-03T23:15:35.788297Z","shell.execute_reply":"2024-02-03T23:15:42.265111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor.set_image(image)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:15:42.267995Z","iopub.execute_input":"2024-02-03T23:15:42.268272Z","iopub.status.idle":"2024-02-03T23:15:44.154487Z","shell.execute_reply.started":"2024-02-03T23:15:42.268248Z","shell.execute_reply":"2024-02-03T23:15:44.153400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_point = np.array([[140, 115]])\ninput_label = np.array([1])","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:17:13.850603Z","iopub.execute_input":"2024-02-03T23:17:13.851569Z","iopub.status.idle":"2024-02-03T23:17:13.856288Z","shell.execute_reply.started":"2024-02-03T23:17:13.851530Z","shell.execute_reply":"2024-02-03T23:17:13.855362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.axis('on')\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:17:14.184621Z","iopub.execute_input":"2024-02-03T23:17:14.184966Z","iopub.status.idle":"2024-02-03T23:17:14.468338Z","shell.execute_reply.started":"2024-02-03T23:17:14.184939Z","shell.execute_reply":"2024-02-03T23:17:14.467305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:17:14.572467Z","iopub.execute_input":"2024-02-03T23:17:14.572962Z","iopub.status.idle":"2024-02-03T23:17:14.587327Z","shell.execute_reply.started":"2024-02-03T23:17:14.572922Z","shell.execute_reply":"2024-02-03T23:17:14.586567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores.max()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:17:16.501437Z","iopub.execute_input":"2024-02-03T23:17:16.501788Z","iopub.status.idle":"2024-02-03T23:17:16.508043Z","shell.execute_reply.started":"2024-02-03T23:17:16.501760Z","shell.execute_reply":"2024-02-03T23:17:16.507074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.figure(figsize=(10,10))\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\n    plt.show()  ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:17:17.365487Z","iopub.execute_input":"2024-02-03T23:17:17.366135Z","iopub.status.idle":"2024-02-03T23:17:18.194564Z","shell.execute_reply.started":"2024-02-03T23:17:17.366101Z","shell.execute_reply":"2024-02-03T23:17:18.193580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_point = np.array([[140, 115] ,[220,115]]) \ninput_label = np.array([1,1])\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:19:29.461830Z","iopub.execute_input":"2024-02-03T23:19:29.462210Z","iopub.status.idle":"2024-02-03T23:19:29.467506Z","shell.execute_reply.started":"2024-02-03T23:19:29.462178Z","shell.execute_reply":"2024-02-03T23:19:29.466435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks, _, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    mask_input=mask_input[None, :, :],\n    multimask_output=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:19:31.942617Z","iopub.execute_input":"2024-02-03T23:19:31.943334Z","iopub.status.idle":"2024-02-03T23:19:31.961794Z","shell.execute_reply.started":"2024-02-03T23:19:31.943297Z","shell.execute_reply":"2024-02-03T23:19:31.960845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_mask(masks, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:19:32.272922Z","iopub.execute_input":"2024-02-03T23:19:32.273255Z","iopub.status.idle":"2024-02-03T23:19:32.559766Z","shell.execute_reply.started":"2024-02-03T23:19:32.273228Z","shell.execute_reply":"2024-02-03T23:19:32.558754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_image = mask_im(masks)\nmask_image = mask_image[:,:,:3]\n\n\nimg_n = cv2.normalize(src=mask_image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:07:27.102427Z","iopub.execute_input":"2024-02-03T23:07:27.103093Z","iopub.status.idle":"2024-02-03T23:07:27.153810Z","shell.execute_reply.started":"2024-02-03T23:07:27.103055Z","shell.execute_reply":"2024-02-03T23:07:27.152568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gray = cv2.cvtColor(img_n , cv2.COLOR_RGB2GRAY)\nplt.imshow(gray, cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:56:31.384541Z","iopub.execute_input":"2024-02-03T22:56:31.384859Z","iopub.status.idle":"2024-02-03T22:56:31.558589Z","shell.execute_reply.started":"2024-02-03T22:56:31.384832Z","shell.execute_reply":"2024-02-03T22:56:31.557713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:56:31.673451Z","iopub.execute_input":"2024-02-03T22:56:31.673980Z","iopub.status.idle":"2024-02-03T22:56:31.678453Z","shell.execute_reply.started":"2024-02-03T22:56:31.673953Z","shell.execute_reply":"2024-02-03T22:56:31.677488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bw = cv2.bitwise_and(original,original, mask=binary_image)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:56:38.111101Z","iopub.execute_input":"2024-02-03T22:56:38.112168Z","iopub.status.idle":"2024-02-03T22:56:38.117103Z","shell.execute_reply.started":"2024-02-03T22:56:38.112122Z","shell.execute_reply":"2024-02-03T22:56:38.116151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(bw)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:56:39.038817Z","iopub.execute_input":"2024-02-03T22:56:39.039805Z","iopub.status.idle":"2024-02-03T22:56:39.218044Z","shell.execute_reply.started":"2024-02-03T22:56:39.039771Z","shell.execute_reply":"2024-02-03T22:56:39.217074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"edit_alone/\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:44:30.899772Z","iopub.execute_input":"2024-02-03T22:44:30.900523Z","iopub.status.idle":"2024-02-03T22:44:30.905372Z","shell.execute_reply.started":"2024-02-03T22:44:30.900487Z","shell.execute_reply":"2024-02-03T22:44:30.904337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/working/edit_alone/\"\ncv2.imwrite(output_path+ \"1.jpg\", bw)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:53:47.630031Z","iopub.execute_input":"2024-02-03T22:53:47.630424Z","iopub.status.idle":"2024-02-03T22:53:47.637681Z","shell.execute_reply.started":"2024-02-03T22:53:47.630392Z","shell.execute_reply":"2024-02-03T22:53:47.636715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(\"/kaggle/working/edit_alone/0.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:46:28.412374Z","iopub.execute_input":"2024-02-03T22:46:28.413610Z","iopub.status.idle":"2024-02-03T22:46:28.419711Z","shell.execute_reply.started":"2024-02-03T22:46:28.413563Z","shell.execute_reply":"2024-02-03T22:46:28.418689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Folder","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torchvision version:\", torchvision.__version__)\nprint(\"CUDA is available:\", torch.cuda.is_available())\nimport sys\n!{sys.executable} -m pip install opencv-python matplotlib\n!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n\n!mkdir images\n!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n\n    #sam_vit_b_01ec64.pth\n!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:25:33.663414Z","iopub.execute_input":"2024-02-03T23:25:33.664410Z","iopub.status.idle":"2024-02-03T23:26:18.948047Z","shell.execute_reply.started":"2024-02-03T23:25:33.664371Z","shell.execute_reply":"2024-02-03T23:26:18.946641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamPredictor\n\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\npredictor = SamPredictor(sam)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:26:18.951062Z","iopub.execute_input":"2024-02-03T23:26:18.951474Z","iopub.status.idle":"2024-02-03T23:26:26.745223Z","shell.execute_reply.started":"2024-02-03T23:26:18.951429Z","shell.execute_reply":"2024-02-03T23:26:26.744155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_mask_bw(path_file):\n    image = cv2.imread(path_file)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original = image\n    predictor.set_image(image)\n    input_point = np.array([[image.shape[0]//2, image.shape[1]//2]])\n    input_label = np.array([1])\n\n    masks, scores, logits = predictor.predict(\n      point_coords=input_point,\n      point_labels=input_label,\n      multimask_output=True,\n    )\n\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        if abs(score - scores.max()) <= 10**-16:\n            mask_image = mask_im(mask)\n\n    mask_image = mask_image[:,:,:3]\n    img_n = cv2.normalize(src=mask_image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    gray = cv2.cvtColor(img_n , cv2.COLOR_RGB2GRAY)\n    binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n    bw = cv2.bitwise_and(original,original, mask=binary_image)\n    return bw","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:26:26.746543Z","iopub.execute_input":"2024-02-03T23:26:26.746939Z","iopub.status.idle":"2024-02-03T23:26:27.338418Z","shell.execute_reply.started":"2024-02-03T23:26:26.746891Z","shell.execute_reply":"2024-02-03T23:26:27.337064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\n\n\n# Specify the input and output folders\ninput_folder = '/kaggle/input/image-search/queries/queries/'\noutput_folder = 'edit_queries/'\n\n# Create the output folder if it doesn't exist\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n\n# Loop through each file in the input folder\nfor filename in os.listdir(input_folder):\n    if filename.endswith(('.jpg', '.png', '.jpeg')):  # Add more file extensions if needed\n        # Load the original image\n        bw = make_mask_bw(input_folder + filename)\n        \n        output_path = os.path.join(output_folder, filename)\n        cv2.imwrite(output_path, bw)\n\nprint(\"Processing complete.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:26:27.341370Z","iopub.execute_input":"2024-02-03T23:26:27.341810Z","iopub.status.idle":"2024-02-03T23:27:13.498287Z","shell.execute_reply.started":"2024-02-03T23:26:27.341770Z","shell.execute_reply":"2024-02-03T23:27:13.497060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = \"/kaggle/working/edit_queries/\"\nfor i in os.listdir(folder):\n    im = cv2.imread(folder + i)\n    plt.imshow(im)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:27:13.500109Z","iopub.execute_input":"2024-02-03T23:27:13.500717Z","iopub.status.idle":"2024-02-03T23:27:18.433662Z","shell.execute_reply.started":"2024-02-03T23:27:13.500676Z","shell.execute_reply":"2024-02-03T23:27:18.432740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}